{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 IMPORTS\n",
    "# --- General-purpose libraries ---\n",
    "import os                     # File and directory operations\n",
    "import pandas as pd            # Data manipulation and analysis\n",
    "import numpy as np             # Numerical computations\n",
    "\n",
    "# --- Visualization libraries ---\n",
    "import seaborn as sns          # Advanced data visualization (heatmaps, boxplots, etc.)\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "\n",
    "# --- Scikit-learn: data splitting and preprocessing ---\n",
    "from sklearn.model_selection import train_test_split  # Split data into train/validation/test sets\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler  # Scaling methods\n",
    "\n",
    "# --- Scikit-learn: feature selection and regression models ---\n",
    "from sklearn.feature_selection import RFE             # Recursive Feature Elimination\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV  # Linear, Ridge, and Lasso regressors\n",
    "from sklearn.ensemble import RandomForestRegressor    # Ensemble-based regressor\n",
    "from sklearn.tree import DecisionTreeRegressor        # Simple tree-based regressor\n",
    "\n",
    "# --- Scikit-learn: classification models ---\n",
    "from sklearn.linear_model import LogisticRegression   # Linear model for classification\n",
    "from sklearn.tree import DecisionTreeClassifier        # Decision tree classifier\n",
    "from sklearn.ensemble import RandomForestClassifier    # Random forest classifier\n",
    "\n",
    "# --- Statistical and diagnostic tools ---\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor  # Variance Inflation Factor (multicollinearity)\n",
    "from scipy.stats import spearmanr                   # Spearman correlation (non-parametric)\n",
    "\n",
    "# --- Scikit-learn: feature selection methods ---\n",
    "from sklearn.feature_selection import mutual_info_regression, f_regression, chi2\n",
    "\n",
    "# --- Scikit-learn: feature selection with regularization ---\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# --- Lasso regression (já tem LassoCV, mas precisa do Lasso básico) ---\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# --- Visualization theme ---\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASET OVERVIEW ===\n",
      "Training set shape: (75973, 14)\n",
      "Test set shape: (32567, 13)\n",
      "Total features: 14\n",
      "Numerical features: 10\n",
      "Categorical features: 4\n",
      "\n",
      "=== SAMPLE DATA ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carID</th>\n",
       "      <th>Brand</th>\n",
       "      <th>model</th>\n",
       "      <th>year</th>\n",
       "      <th>price</th>\n",
       "      <th>transmission</th>\n",
       "      <th>mileage</th>\n",
       "      <th>fuelType</th>\n",
       "      <th>tax</th>\n",
       "      <th>mpg</th>\n",
       "      <th>engineSize</th>\n",
       "      <th>paintQuality%</th>\n",
       "      <th>previousOwners</th>\n",
       "      <th>hasDamage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69512</td>\n",
       "      <td>VW</td>\n",
       "      <td>Golf</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>22290</td>\n",
       "      <td>Semi-Auto</td>\n",
       "      <td>28421.0</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.417268</td>\n",
       "      <td>2.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53000</td>\n",
       "      <td>Toyota</td>\n",
       "      <td>Yaris</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>13790</td>\n",
       "      <td>Manual</td>\n",
       "      <td>4589.0</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>145.0</td>\n",
       "      <td>47.900000</td>\n",
       "      <td>1.5</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6366</td>\n",
       "      <td>Audi</td>\n",
       "      <td>Q2</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>24990</td>\n",
       "      <td>Semi-Auto</td>\n",
       "      <td>3624.0</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>145.0</td>\n",
       "      <td>40.900000</td>\n",
       "      <td>1.5</td>\n",
       "      <td>56.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29021</td>\n",
       "      <td>Ford</td>\n",
       "      <td>FIESTA</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>12500</td>\n",
       "      <td>anual</td>\n",
       "      <td>9102.0</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>145.0</td>\n",
       "      <td>65.700000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-2.340306</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10062</td>\n",
       "      <td>BMW</td>\n",
       "      <td>2 Series</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>22995</td>\n",
       "      <td>Manual</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>145.0</td>\n",
       "      <td>42.800000</td>\n",
       "      <td>1.5</td>\n",
       "      <td>97.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carID   Brand      model    year  price transmission  mileage fuelType  \\\n",
       "0  69512      VW       Golf  2016.0  22290    Semi-Auto  28421.0   Petrol   \n",
       "1  53000  Toyota      Yaris  2019.0  13790       Manual   4589.0   Petrol   \n",
       "2   6366    Audi         Q2  2019.0  24990    Semi-Auto   3624.0   Petrol   \n",
       "3  29021    Ford     FIESTA  2018.0  12500        anual   9102.0   Petrol   \n",
       "4  10062     BMW   2 Series  2019.0  22995       Manual   1000.0   Petrol   \n",
       "\n",
       "     tax        mpg  engineSize  paintQuality%  previousOwners  hasDamage  \n",
       "0    NaN  11.417268         2.0           63.0        4.000000        0.0  \n",
       "1  145.0  47.900000         1.5           50.0        1.000000        0.0  \n",
       "2  145.0  40.900000         1.5           56.0        4.000000        0.0  \n",
       "3  145.0  65.700000         1.0           50.0       -2.340306        0.0  \n",
       "4  145.0  42.800000         1.5           97.0        3.000000        0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 📂 LOAD RAW DATA\n",
    "train_relative_path = '../Data/train.csv'\n",
    "test_relative_path = '../Data/test.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_relative_path)\n",
    "test_data = pd.read_csv(test_relative_path)\n",
    "\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Training set shape: {train_data.shape}\")\n",
    "print(f\"Test set shape: {test_data.shape}\")\n",
    "print(f\"Total features: {len(train_data.columns)}\")\n",
    "\n",
    "numerical_features = train_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = train_data.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Numerical features: {len(numerical_features)}\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")\n",
    "\n",
    "print(\"\\n=== SAMPLE DATA ===\")\n",
    "display(train_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 numeric columns present in both datasets.\n",
      "File already exists: [ML]_Project_EDAOutputs_Group45/Numeric_Variables_Histograms_Boxplots.png. Skipping plot generation.\n"
     ]
    }
   ],
   "source": [
    "# 📊 NUMERIC FEATURE DISTRIBUTIONS\n",
    "output_dir = \"[ML]_Project_EDAOutputs_Group45\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_file = os.path.join(output_dir, \"Numeric_Variables_Histograms_Boxplots.png\")\n",
    "palette = sns.color_palette(\"Spectral\", 8)\n",
    "\n",
    "numeric_train = train_data.select_dtypes(include=[np.number]).columns\n",
    "numeric_test = test_data.select_dtypes(include=[np.number]).columns\n",
    "metric_cols = [col for col in numeric_train if col in numeric_test]\n",
    "\n",
    "print(f\"Found {len(metric_cols)} numeric columns present in both datasets.\")\n",
    "\n",
    "if os.path.isfile(output_file):\n",
    "    print(f\"File already exists: {output_file}. Skipping plot generation.\")\n",
    "else:\n",
    "    print(f\"Generating plot for {len(metric_cols)} numeric variables...\")\n",
    "\n",
    "    sp_cols = 5\n",
    "    sp_rows = (len(metric_cols) + sp_cols - 1) // sp_cols\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        sp_rows * 2, sp_cols,\n",
    "        figsize=(20, 6 * sp_rows),\n",
    "        tight_layout=False,\n",
    "        gridspec_kw={'height_ratios': [0.2, 0.8] * sp_rows}\n",
    "    )\n",
    "\n",
    "    for i, (ax_box, ax_hist, feat) in enumerate(zip(axes[::2].flatten(), axes[1::2].flatten(), metric_cols)):\n",
    "        if feat not in train_data.columns or feat not in test_data.columns:\n",
    "            continue\n",
    "\n",
    "        data = pd.concat([train_data[[feat]], test_data[[feat]]], axis=1)\n",
    "        data.columns = ['Train', 'Test']\n",
    "\n",
    "        sns.boxplot(data=data[['Train', 'Test']], palette=[palette[0], palette[4]], orient='h', ax=ax_box)\n",
    "        ax_box.set_xlabel(None)\n",
    "        ax_box.set_ylabel(None)\n",
    "\n",
    "        sns.histplot(train_data[feat], color=palette[0], kde=True, stat='percent', bins=50, alpha=0.4, ax=ax_hist, label='Train')\n",
    "        sns.histplot(test_data[feat], color=palette[4], kde=True, stat='percent', bins=50, alpha=0.3, ax=ax_hist, label='Test')\n",
    "\n",
    "        for data_type, color_idx, line_style, alpha_val in [('Train', 2, '--', 0.8), ('Test', 2, '-', 0.5)]:\n",
    "            vals = train_data[feat] if data_type == 'Train' else test_data[feat]\n",
    "            ax_hist.axvline(vals.mean(), color=palette[color_idx], linestyle=line_style, linewidth=1.5, alpha=alpha_val, label=f'{data_type} Mean: {vals.mean():.1f}')\n",
    "            ax_hist.axvline(vals.median(), color=palette[color_idx+1], linestyle=line_style, linewidth=1.5, alpha=alpha_val, label=f'{data_type} Median: {vals.median():.1f}')\n",
    "\n",
    "        ax_hist.set_title(feat, y=-0.20, fontweight='bold')\n",
    "        ax_hist.set_xlabel(None)\n",
    "        if i % sp_cols == 0:\n",
    "            ax_hist.set_ylabel('Count (n)\\n', fontsize=10, fontweight='bold')\n",
    "        else:\n",
    "            ax_hist.set_ylabel(None)\n",
    "\n",
    "        sns.despine(top=True, right=True, ax=ax_hist)\n",
    "        sns.despine(top=True, right=True, bottom=True, ax=ax_box)\n",
    "        ax_hist.legend(fontsize=7, frameon=False, loc='best')\n",
    "\n",
    "    for j in range(len(metric_cols)*2, len(axes)):\n",
    "        fig.delaxes(axes.flatten()[j])\n",
    "\n",
    "    plt.suptitle(\"Numeric Variables' Histograms with Boxplots\", fontweight='bold', fontsize=16)\n",
    "    fig.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"✅ Plot saved as: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Clean data loaded successfully. Shape: (75973, 16)\n"
     ]
    }
   ],
   "source": [
    "# 🧹 LOAD CLEANED DATA AND SPLIT\n",
    "clean_train_df = pd.read_csv(\"../Data/clean_data_train.csv\")\n",
    "print(f\"✅ Clean data loaded successfully. Shape: {clean_train_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carID', 'Brand', 'model', 'year', 'transmission', 'mileage', 'fuelType', 'tax', 'mpg', 'engineSize', 'paintQuality%', 'previousOwners', 'hasDamage', 'Brand_cleaned', 'Brand_confidence', 'price']\n"
     ]
    }
   ],
   "source": [
    "print(clean_train_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training shape: (60778, 15)\n",
      "✅ Validation shape: (15195, 15)\n",
      "📊 Numeric features: 13 -> ['carID', 'year', 'transmission', 'mileage', 'fuelType', 'tax', 'mpg', 'engineSize', 'paintQuality%', 'previousOwners', 'hasDamage', 'Brand_cleaned', 'Brand_confidence']\n",
      "🔤 Categorical features: 2 -> ['Brand', 'model']\n"
     ]
    }
   ],
   "source": [
    "target_col = \"price\"\n",
    "X = clean_train_df.drop(columns=[target_col])\n",
    "y = clean_train_df[target_col]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"✅ Training shape: {X_train.shape}\")\n",
    "print(f\"✅ Validation shape: {X_val.shape}\")\n",
    "\n",
    "numeric_features = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "print(f\"📊 Numeric features: {len(numeric_features)} -> {numeric_features}\")\n",
    "print(f\"🔤 Categorical features: {len(categorical_features)} -> {categorical_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 FEATURE SELECTION CLASS\n",
    "class NumericalFeatureSelector:\n",
    "    \"\"\"\n",
    "    Classe para seleção de features numéricas em problemas de regressão.\n",
    "    Usa apenas dados de treino para evitar data leakage.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X_train, y_train, numeric_features, X_val=None, y_val=None, vif_threshold=5, corr_threshold=0.7):\n",
    "        self.X_train = X_train[numeric_features].copy()\n",
    "        self.y_train = y_train.copy()\n",
    "        self.X_val = X_val[numeric_features].copy() if X_val is not None else None\n",
    "        self.y_val = y_val.copy() if y_val is not None else None\n",
    "        self.numeric_features = numeric_features\n",
    "        self.vif_threshold = vif_threshold\n",
    "        self.corr_threshold = corr_threshold\n",
    "\n",
    "    # 1️⃣ Multicolinearidade / Redundância\n",
    "    def vif_analysis(self):\n",
    "        X = self.X_train.dropna().copy()\n",
    "        vif_data = pd.DataFrame()\n",
    "        vif_data[\"Feature\"] = X.columns\n",
    "        vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "        vif_data[\"Accepted\"] = vif_data[\"VIF\"] < self.vif_threshold\n",
    "        return vif_data\n",
    "\n",
    "    def spearman_redundancy(self):\n",
    "        corr = self.X_train.corr(method='spearman').abs()\n",
    "        upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "        redundancy_df = pd.DataFrame({\n",
    "            \"Feature\": self.X_train.columns,\n",
    "            \"Max_SpearmanCorr\": [upper[col].max(skipna=True) for col in self.X_train.columns],\n",
    "        })\n",
    "        redundancy_df[\"Accepted\"] = redundancy_df[\"Max_SpearmanCorr\"] < self.corr_threshold\n",
    "        return redundancy_df\n",
    "\n",
    "    # 2️⃣ Correlação com o Target (Relevância)\n",
    "    def spearman_relevance(self, threshold=0.1):\n",
    "        corr_values = []\n",
    "        for col in self.X_train.columns:\n",
    "            corr, _ = spearmanr(self.X_train[col], self.y_train)\n",
    "            corr_values.append(abs(corr))\n",
    "        corr_df = pd.DataFrame({\n",
    "            \"Feature\": self.X_train.columns,\n",
    "            \"Spearman_TargetCorr\": corr_values\n",
    "        })\n",
    "        corr_df[\"Accepted\"] = corr_df[\"Spearman_TargetCorr\"] > threshold\n",
    "        return corr_df\n",
    "\n",
    "    # 3️⃣ Recursive Feature Elimination (RFE)\n",
    "    def rfe_model(self, model, scaler=None):\n",
    "        X = self.X_train.copy()\n",
    "        if scaler:\n",
    "            X = scaler.fit_transform(X)\n",
    "        rfe = RFE(model)\n",
    "        rfe.fit(X, self.y_train)\n",
    "        results = pd.DataFrame({\"Feature\": self.X_train.columns, \"Accepted\": rfe.support_})\n",
    "        return results\n",
    "\n",
    "    def rfe_all_models(self):\n",
    "        models = {\n",
    "            \"RFE_DecisionTree\": DecisionTreeRegressor(random_state=42),\n",
    "            \"RFE_RandomForest\": RandomForestRegressor(random_state=42, n_estimators=100),\n",
    "            \"RFE_LR_MinMax\": (LinearRegression(), MinMaxScaler()),\n",
    "            \"RFE_LR_Standard\": (LinearRegression(), StandardScaler()),\n",
    "            \"RFE_LR_Robust\": (LinearRegression(), RobustScaler())\n",
    "        }\n",
    "        results = []\n",
    "        for name, model in models.items():\n",
    "            df = self.rfe_model(model[0], model[1]) if isinstance(model, tuple) else self.rfe_model(model)\n",
    "            df = df.rename(columns={\"Accepted\": name})\n",
    "            df = df[[\"Feature\", name]]\n",
    "            results.append(df)\n",
    "        return results\n",
    "\n",
    "    # 4️⃣ Regularização Ridge/Lasso\n",
    "    def regularization_model(self, model_type=\"ridge\", scaler=None, threshold=0.01):\n",
    "        X = self.X_train.copy()\n",
    "        if scaler:\n",
    "            X = scaler.fit_transform(X)\n",
    "        model = RidgeCV(alphas=np.logspace(-3, 3, 50)) if model_type == \"ridge\" else LassoCV(alphas=np.logspace(-3, 3, 50), max_iter=10000)\n",
    "        model.fit(X, self.y_train)\n",
    "        coefs = np.abs(model.coef_)\n",
    "        df = pd.DataFrame({\n",
    "            \"Feature\": self.X_train.columns,\n",
    "            f\"{model_type.capitalize()}_Coef\": coefs,\n",
    "            \"Accepted\": coefs > threshold\n",
    "        })\n",
    "        return df\n",
    "\n",
    "    def ridge_all(self):\n",
    "        scalers = {\"Ridge_MinMax\": MinMaxScaler(), \"Ridge_Standard\": StandardScaler(), \"Ridge_Robust\": RobustScaler()}\n",
    "        results = []\n",
    "        for name, scaler in scalers.items():\n",
    "            df = self.regularization_model(\"ridge\", scaler)\n",
    "            df = df.rename(columns={\"Accepted\": name})\n",
    "            df = df[[\"Feature\", name]]\n",
    "            results.append(df)\n",
    "        return results\n",
    "\n",
    "    def lasso_all(self):\n",
    "        scalers = {\"Lasso_MinMax\": MinMaxScaler(), \"Lasso_Standard\": StandardScaler(), \"Lasso_Robust\": RobustScaler()}\n",
    "        results = []\n",
    "        for name, scaler in scalers.items():\n",
    "            df = self.regularization_model(\"lasso\", scaler)\n",
    "            df = df.rename(columns={\"Accepted\": name})\n",
    "            df = df[[\"Feature\", name]]\n",
    "            results.append(df)\n",
    "        return results\n",
    "\n",
    "    # 5️⃣ Tabela Final\n",
    "    def compile_results(self):\n",
    "        results = [\n",
    "            self.vif_analysis(),\n",
    "            self.spearman_redundancy(),\n",
    "            self.spearman_relevance(),\n",
    "            *self.rfe_all_models(),\n",
    "            *self.ridge_all(),\n",
    "            *self.lasso_all()\n",
    "        ]\n",
    "        merged = results[0][[\"Feature\"]]\n",
    "        for df in results:\n",
    "            merged = merged.merge(df, on=\"Feature\", how=\"left\")\n",
    "\n",
    "        accept_cols = [c for c in merged.columns if \"RFE_\" in c or \"Ridge_\" in c or \"Lasso_\" in c or \"Accepted\" in c]\n",
    "        merged[\"Total_Accepted\"] = merged[accept_cols].sum(axis=1)\n",
    "        merged[\"Final_Decision\"] = np.where(merged[\"Total_Accepted\"] > len(accept_cols) / 2, \"Keep\", \"Drop\")\n",
    "        return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m spearman_redundancy \u001b[38;5;241m=\u001b[39m fs\u001b[38;5;241m.\u001b[39mspearman_redundancy()\n\u001b[1;32m     16\u001b[0m spearman_relevance \u001b[38;5;241m=\u001b[39m fs\u001b[38;5;241m.\u001b[39mspearman_relevance(threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m rfe_results \u001b[38;5;241m=\u001b[39m fs\u001b[38;5;241m.\u001b[39mrfe_all_models()\n\u001b[1;32m     19\u001b[0m ridge_results \u001b[38;5;241m=\u001b[39m fs\u001b[38;5;241m.\u001b[39mridge_all()\n\u001b[1;32m     20\u001b[0m lasso_results \u001b[38;5;241m=\u001b[39m fs\u001b[38;5;241m.\u001b[39mlasso_all()\n",
      "Cell \u001b[0;32mIn[12], line 69\u001b[0m, in \u001b[0;36mNumericalFeatureSelector.rfe_all_models\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 69\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrfe_model(model[\u001b[38;5;241m0\u001b[39m], model[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrfe_model(model)\n\u001b[1;32m     70\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccepted\u001b[39m\u001b[38;5;124m\"\u001b[39m: name})\n\u001b[1;32m     71\u001b[0m     df \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature\u001b[39m\u001b[38;5;124m\"\u001b[39m, name]]\n",
      "Cell \u001b[0;32mIn[12], line 55\u001b[0m, in \u001b[0;36mNumericalFeatureSelector.rfe_model\u001b[0;34m(self, model, scaler)\u001b[0m\n\u001b[1;32m     53\u001b[0m     X \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X)\n\u001b[1;32m     54\u001b[0m rfe \u001b[38;5;241m=\u001b[39m RFE(model)\n\u001b[0;32m---> 55\u001b[0m rfe\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_train)\n\u001b[1;32m     56\u001b[0m results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_train\u001b[38;5;241m.\u001b[39mcolumns, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccepted\u001b[39m\u001b[38;5;124m\"\u001b[39m: rfe\u001b[38;5;241m.\u001b[39msupport_})\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/anaconda3/envs/DmEnv/lib/python3.11/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/DmEnv/lib/python3.11/site-packages/sklearn/feature_selection/_rfe.py:276\u001b[0m, in \u001b[0;36mRFE.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     routed_params \u001b[38;5;241m=\u001b[39m Bunch(estimator\u001b[38;5;241m=\u001b[39mBunch(fit\u001b[38;5;241m=\u001b[39mfit_params))\n\u001b[0;32m--> 276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit)\n",
      "File \u001b[0;32m~/anaconda3/envs/DmEnv/lib/python3.11/site-packages/sklearn/feature_selection/_rfe.py:332\u001b[0m, in \u001b[0;36mRFE._fit\u001b[0;34m(self, X, y, step_score, **fit_params)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting estimator with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m features.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m np\u001b[38;5;241m.\u001b[39msum(support_))\n\u001b[0;32m--> 332\u001b[0m estimator\u001b[38;5;241m.\u001b[39mfit(X[:, features], y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    334\u001b[0m \u001b[38;5;66;03m# Get importance and rank them\u001b[39;00m\n\u001b[1;32m    335\u001b[0m importances \u001b[38;5;241m=\u001b[39m _get_feature_importances(\n\u001b[1;32m    336\u001b[0m     estimator,\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimportance_getter,\n\u001b[1;32m    338\u001b[0m     transform_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msquare\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    339\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/DmEnv/lib/python3.11/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/DmEnv/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:487\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    476\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    479\u001b[0m ]\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 487\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[1;32m    488\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[1;32m    489\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    490\u001b[0m     prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    491\u001b[0m )(\n\u001b[1;32m    492\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[1;32m    493\u001b[0m         t,\n\u001b[1;32m    494\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbootstrap,\n\u001b[1;32m    495\u001b[0m         X,\n\u001b[1;32m    496\u001b[0m         y,\n\u001b[1;32m    497\u001b[0m         sample_weight,\n\u001b[1;32m    498\u001b[0m         i,\n\u001b[1;32m    499\u001b[0m         \u001b[38;5;28mlen\u001b[39m(trees),\n\u001b[1;32m    500\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    501\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[1;32m    502\u001b[0m         n_samples_bootstrap\u001b[38;5;241m=\u001b[39mn_samples_bootstrap,\n\u001b[1;32m    503\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[1;32m    504\u001b[0m     )\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[1;32m    506\u001b[0m )\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/anaconda3/envs/DmEnv/lib/python3.11/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m~/anaconda3/envs/DmEnv/lib/python3.11/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/anaconda3/envs/DmEnv/lib/python3.11/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/anaconda3/envs/DmEnv/lib/python3.11/site-packages/sklearn/utils/parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/DmEnv/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:189\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    187\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 189\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[1;32m    190\u001b[0m         X,\n\u001b[1;32m    191\u001b[0m         y,\n\u001b[1;32m    192\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39mcurr_sample_weight,\n\u001b[1;32m    193\u001b[0m         check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    194\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[1;32m    195\u001b[0m     )\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[1;32m    198\u001b[0m         X,\n\u001b[1;32m    199\u001b[0m         y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[1;32m    203\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/DmEnv/lib/python3.11/site-packages/sklearn/tree/_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    463\u001b[0m         splitter,\n\u001b[1;32m    464\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m builder\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Inicialização da classe FeatureSelector\n",
    "\n",
    "fs = NumericalFeatureSelector(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    numeric_features=numeric_features,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val\n",
    ")\n",
    "\n",
    "# ================================================================\n",
    "# 🔍 Apply Selection Methods\n",
    "# ================================================================\n",
    "vif_results = fs.vif_analysis()\n",
    "spearman_redundancy = fs.spearman_redundancy()\n",
    "spearman_relevance = fs.spearman_relevance(threshold=0.1)\n",
    "\n",
    "rfe_results = fs.rfe_all_models()\n",
    "ridge_results = fs.ridge_all()\n",
    "lasso_results = fs.lasso_all()\n",
    "\n",
    "# ================================================================\n",
    "# 🧾 Compile Final Table\n",
    "# ================================================================\n",
    "final_results = fs.compile_results()\n",
    "\n",
    "print(\"\\n=== 🔹 Feature Selection Summary ===\")\n",
    "display(final_results.head(15))\n",
    "\n",
    "# ================================================================\n",
    "# 💾 (Optional) Save results\n",
    "# ================================================================\n",
    "final_results.to_csv(\"../Data/feature_selection_summary.csv\", index=False)\n",
    "print(\"✅ Feature selection summary saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 FEATURE SELECTION CLASS FOR ENCODED CATEGORICAL FEATURES\n",
    "class CategoricalFeatureSelector:\n",
    "\n",
    "    def __init__(self, X_train, y_train, categorical_encoded_features, X_val=None, y_val=None, \n",
    "                 corr_threshold=0.8, importance_threshold=0.01):\n",
    "        self.X_train = X_train[categorical_encoded_features].copy()\n",
    "        self.y_train = y_train.copy()\n",
    "        self.X_val = X_val[categorical_encoded_features].copy() if X_val is not None else None\n",
    "        self.y_val = y_val.copy() if y_val is not None else None\n",
    "        self.categorical_encoded_features = categorical_encoded_features\n",
    "        self.corr_threshold = corr_threshold\n",
    "        self.importance_threshold = importance_threshold\n",
    "\n",
    "    # 1️⃣ Multicollinearity / Redundancy\n",
    "    def correlation_redundancy(self):\n",
    "        \"\"\"Detects highly correlated dummy features\"\"\"\n",
    "        corr = self.X_train.corr().abs()\n",
    "        upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "        \n",
    "        redundancy_df = pd.DataFrame({\n",
    "            \"Feature\": self.X_train.columns,\n",
    "            \"Max_PearsonCorr\": [upper[col].max(skipna=True) for col in self.X_train.columns],\n",
    "        })\n",
    "        redundancy_df[\"Low_Correlation\"] = redundancy_df[\"Max_PearsonCorr\"] < self.corr_threshold\n",
    "        return redundancy_df\n",
    "\n",
    "    # 2️⃣ Target Relevance\n",
    "    def mutual_information_relevance(self, threshold=0.01):\n",
    "        \"\"\"Mutual Information for non-linear relationships with categorical data\"\"\"\n",
    "        mi_scores = mutual_info_regression(self.X_train, self.y_train, random_state=42)\n",
    "        \n",
    "        mi_df = pd.DataFrame({\n",
    "            \"Feature\": self.X_train.columns,\n",
    "            \"Mutual_Information\": mi_scores\n",
    "        })\n",
    "        mi_df[\"High_MI\"] = mi_df[\"Mutual_Information\"] > threshold\n",
    "        return mi_df\n",
    "\n",
    "    # 3️⃣ Recursive Feature Elimination (RFE)\n",
    "    def rfe_model(self, model, scaler=None):\n",
    "        X = self.X_train.copy()\n",
    "        if scaler:\n",
    "            X = scaler.fit_transform(X)\n",
    "        rfe = RFE(model)\n",
    "        rfe.fit(X, self.y_train)\n",
    "        results = pd.DataFrame({\"Feature\": self.X_train.columns, \"Accepted\": rfe.support_})\n",
    "        return results\n",
    "\n",
    "    def rfe_all_models(self):\n",
    "        \"\"\"RFE with different models and scalers\"\"\"\n",
    "        models = {\n",
    "            \"RFE_DecisionTree\": DecisionTreeRegressor(random_state=42),\n",
    "            \"RFE_RandomForest\": RandomForestRegressor(random_state=42, n_estimators=100),\n",
    "            \"RFE_LR_MinMax\": (LinearRegression(), MinMaxScaler()),\n",
    "            \"RFE_LR_Standard\": (LinearRegression(), StandardScaler()),\n",
    "            \"RFE_LR_Robust\": (LinearRegression(), RobustScaler())\n",
    "        }\n",
    "        results = []\n",
    "        for name, model in models.items():\n",
    "            df = self.rfe_model(model[0], model[1]) if isinstance(model, tuple) else self.rfe_model(model)\n",
    "            df = df.rename(columns={\"Accepted\": name})\n",
    "            results.append(df)\n",
    "        return results\n",
    "\n",
    "    # 4️⃣ Lasso Regularization\n",
    "    def lasso_model(self, scaler=None, threshold=0.01):\n",
    "        \"\"\"Lasso is excellent for dummy variable selection\"\"\"\n",
    "        X = self.X_train.copy()\n",
    "        if scaler:\n",
    "            X = scaler.fit_transform(X)\n",
    "        \n",
    "        lasso = LassoCV(alphas=np.logspace(-3, 3, 50), max_iter=10000, random_state=42)\n",
    "        lasso.fit(X, self.y_train)\n",
    "        \n",
    "        coefs = np.abs(lasso.coef_)\n",
    "        df = pd.DataFrame({\n",
    "            \"Feature\": self.X_train.columns,\n",
    "            \"Lasso_Coef\": coefs,\n",
    "            \"Accepted\": coefs > threshold\n",
    "        })\n",
    "        return df\n",
    "\n",
    "    def lasso_all_scalers(self):\n",
    "        \"\"\"Lasso with different scaling methods\"\"\"\n",
    "        scalers = {\n",
    "            \"Lasso_MinMax\": MinMaxScaler(),\n",
    "            \"Lasso_Standard\": StandardScaler(), \n",
    "            \"Lasso_Robust\": RobustScaler()\n",
    "        }\n",
    "        results = []\n",
    "        for name, scaler in scalers.items():\n",
    "            df = self.lasso_model(scaler)\n",
    "            df = df.rename(columns={\"Accepted\": name})\n",
    "            results.append(df)\n",
    "        return results\n",
    "\n",
    "    # 5️⃣ Final Results Table (SAME STYLE AS NUMERICAL)\n",
    "    def compile_results(self):\n",
    "        \"\"\"Compile all results into final decision table\"\"\"\n",
    "        results = [\n",
    "            self.correlation_redundancy(),\n",
    "            self.mutual_information_relevance(),\n",
    "            *self.rfe_all_models(),\n",
    "            *self.lasso_all_scalers()\n",
    "        ]\n",
    "        merged = results[0][[\"Feature\"]]\n",
    "        for df in results:\n",
    "            merged = merged.merge(df, on=\"Feature\", how=\"left\")\n",
    "\n",
    "        accept_cols = [c for c in merged.columns if \"RFE_\" in c or \"Lasso_\" in c or \"Low_Correlation\" in c or \"High_MI\" in c]\n",
    "        merged[\"Total_Accepted\"] = merged[accept_cols].sum(axis=1)\n",
    "        merged[\"Final_Decision\"] = np.where(merged[\"Total_Accepted\"] > len(accept_cols) / 2, \"Keep\", \"Drop\")\n",
    "        return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Criando dataset dummy...\n",
      "📊 Dataset criado: (50, 11)\n",
      "🎯 Features disponíveis: ['Brand_Audi', 'Brand_BMW', 'Brand_Mercedes', 'Brand_Ford', 'Brand_Toyota', 'model_A4', 'model_320i', 'model_CClass', 'model_Focus', 'model_Corolla', 'price']\n",
      "\n",
      "🔤 Features categóricas encoded: 10\n",
      "📈 Target: price\n"
     ]
    }
   ],
   "source": [
    "print(\"🧪 Criando dataset dummy...\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# Criar 50 linhas com features categóricas encoded\n",
    "dummy_data = pd.DataFrame({\n",
    "    # Features categóricas encoded (dummy variables)\n",
    "    'Brand_Audi': np.random.choice([0, 1], 50, p=[0.7, 0.3]),\n",
    "    'Brand_BMW': np.random.choice([0, 1], 50, p=[0.7, 0.3]),\n",
    "    'Brand_Mercedes': np.random.choice([0, 1], 50, p=[0.8, 0.2]),\n",
    "    'Brand_Ford': np.random.choice([0, 1], 50, p=[0.6, 0.4]),\n",
    "    'Brand_Toyota': np.random.choice([0, 1], 50, p=[0.5, 0.5]),\n",
    "    'model_A4': np.random.choice([0, 1], 50, p=[0.8, 0.2]),\n",
    "    'model_320i': np.random.choice([0, 1], 50, p=[0.85, 0.15]),\n",
    "    'model_CClass': np.random.choice([0, 1], 50, p=[0.9, 0.1]),\n",
    "    'model_Focus': np.random.choice([0, 1], 50, p=[0.7, 0.3]),\n",
    "    'model_Corolla': np.random.choice([0, 1], 50, p=[0.6, 0.4]),\n",
    "    \n",
    "    # Target (preço do carro) - criar alguma relação com as features\n",
    "    'price': np.random.normal(25000, 8000, 50)\n",
    "})\n",
    "\n",
    "# Adicionar alguma relação real entre features e target para tornar o teste mais realista\n",
    "dummy_data.loc[dummy_data['Brand_Audi'] == 1, 'price'] += 5000\n",
    "dummy_data.loc[dummy_data['Brand_BMW'] == 1, 'price'] += 4000\n",
    "dummy_data.loc[dummy_data['model_A4'] == 1, 'price'] += 3000\n",
    "dummy_data.loc[dummy_data['model_Focus'] == 1, 'price'] -= 2000\n",
    "\n",
    "print(f\"📊 Dataset criado: {dummy_data.shape}\")\n",
    "print(f\"🎯 Features disponíveis: {list(dummy_data.columns)}\")\n",
    "\n",
    "# 🎯 DEFINIR FEATURES E TARGET\n",
    "categorical_encoded_features = [col for col in dummy_data.columns if col.startswith(('Brand_', 'model_'))]\n",
    "X_dummy = dummy_data[categorical_encoded_features]\n",
    "y_dummy = dummy_data['price']\n",
    "\n",
    "print(f\"\\n🔤 Features categóricas encoded: {len(categorical_encoded_features)}\")\n",
    "print(f\"📈 Target: price\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Feature  Max_PearsonCorr  Low_Correlation  Mutual_Information  \\\n",
      "0      Brand_Audi              NaN            False            0.144147   \n",
      "1       Brand_BMW         0.017904             True            0.000000   \n",
      "2  Brand_Mercedes         0.182239             True            0.000000   \n",
      "3      Brand_Ford         0.187931             True            0.000000   \n",
      "4    Brand_Toyota         0.210090             True            0.000000   \n",
      "5        model_A4         0.123613             True            0.028531   \n",
      "6      model_320i         0.332674             True            0.036110   \n",
      "7    model_CClass         0.340825             True            0.000000   \n",
      "8     model_Focus         0.151104             True            0.163585   \n",
      "9   model_Corolla         0.394771             True            0.106070   \n",
      "\n",
      "   High_MI  RFE_DecisionTree  RFE_RandomForest  RFE_LR_MinMax  \\\n",
      "0     True              True              True           True   \n",
      "1    False             False              True          False   \n",
      "2    False              True             False          False   \n",
      "3    False              True              True          False   \n",
      "4    False              True              True          False   \n",
      "5     True             False             False           True   \n",
      "6     True             False             False           True   \n",
      "7    False             False             False           True   \n",
      "8     True             False             False          False   \n",
      "9     True              True              True           True   \n",
      "\n",
      "   RFE_LR_Standard  RFE_LR_Robust  Lasso_Coef_x  Lasso_MinMax  Lasso_Coef_y  \\\n",
      "0             True           True   6343.574353          True   3278.802888   \n",
      "1            False          False   2076.284575          True    890.142368   \n",
      "2            False          False      0.000000         False      0.000000   \n",
      "3            False          False      0.000000         False    327.801048   \n",
      "4            False          False     53.929947          True    127.891573   \n",
      "5             True           True    474.992638          True    919.632262   \n",
      "6             True           True      0.000000         False   1055.752631   \n",
      "7             True           True   2475.142963          True   1522.915741   \n",
      "8            False          False      0.000000         False      0.000000   \n",
      "9             True           True   1307.749588          True   1250.980111   \n",
      "\n",
      "   Lasso_Standard   Lasso_Coef  Lasso_Robust Total_Accepted Final_Decision  \n",
      "0            True  6343.574353          True   15974.951594           Keep  \n",
      "1            True  2076.284575          True    5047.711518           Keep  \n",
      "2           False     0.000000         False            2.0           Drop  \n",
      "3            True     0.000000         False     331.801048           Keep  \n",
      "4            True    53.929947          True     241.751467           Keep  \n",
      "5            True   474.992638          True    1877.617538           Keep  \n",
      "6            True     0.000000         False    1061.752631           Keep  \n",
      "7            True  2475.142963          True    6480.201667           Keep  \n",
      "8           False     0.000000         False            2.0           Drop  \n",
      "9            True  1307.749588          True    3876.479286           Keep  \n"
     ]
    }
   ],
   "source": [
    "# Teste final\n",
    "cat_selector = CategoricalFeatureSelector(\n",
    "    X_train=X_dummy,\n",
    "    y_train=y_dummy,\n",
    "    categorical_encoded_features=categorical_encoded_features,\n",
    "    corr_threshold=0.8,\n",
    "    importance_threshold=0.001\n",
    ")\n",
    "\n",
    "results = cat_selector.compile_results()\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DmEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
